{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83219635",
   "metadata": {},
   "source": [
    "Date - 12/10/2021  11:30 AM to 01:30 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e64ecd",
   "metadata": {},
   "source": [
    "# Ensemble Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974cdfb",
   "metadata": {},
   "source": [
    "    Ensemble model in Machine Learning operate on multiple models to improve the overall performance.\n",
    "    \n",
    "**This can be achieved in various ways:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18031633",
   "metadata": {},
   "source": [
    "### 1). Max Voting:\n",
    "\n",
    "    a). The max voting method is generally used for classification problem.\n",
    "    b). In this technique multiple models are used to make predictions for each data points.\n",
    "    c). The prediction by each model are considered as vote.\n",
    "    d). The prediction which we get from the majority of the models are useful as the final prediction.\n",
    "    \n",
    "    Example:- When you ask your friends to suggest you for a mobile phone, there will be multiple options being suggested,\n",
    "                \n",
    "                Friend 1 -> Iphone\n",
    "                Friend 2 -> Oppo\n",
    "                Friend 3 -> One plus\n",
    "                Friend 4 -> One Plus\n",
    "                Friend 5 -> Red Mi\n",
    "                \n",
    "    - In this case the maximum vote is for One Plus hence the decision of purchasing one plus is taken by you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70590e4c",
   "metadata": {},
   "source": [
    "### 2). Averaging:\n",
    "\n",
    "    a). Similar to Max voting technique, multiple predictions are made for each data points in averaging.\n",
    "    b). In this method we take an average of predictions from all the models and use it to make final predicitions.\n",
    "    c). Averaging can be used for making predicitons in regression problem or while calculating the probablities for\n",
    "        classification problems.\n",
    "    \n",
    "    Example:- You have asked for opinion to purchase mobile phone from 5 friends,\n",
    "        \n",
    "                F1 -> IPhone\n",
    "                F2 -> Red Mi\n",
    "                F3 -> One Plus\n",
    "                F4 -> One Plus\n",
    "                F5 -> Oppo\n",
    "                \n",
    "    - Here the average would be take:-\n",
    "    \n",
    "                (1 + 1 + 2 + 1)/5\n",
    "    \n",
    "    - And the final value would be the predictive value for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba86502",
   "metadata": {},
   "source": [
    "### 3). Weighted Average:\n",
    "\n",
    "    a). Weighted Average Technique is an extension of the average method.\n",
    "    b). All models are assigned different weights, defining the importance of each model for prediction.\n",
    "    \n",
    "    - For instance if 4 of your friends have prior experience in using mobile phone and while 3 of them have no prior\n",
    "      experience of using mobile phone.\n",
    "    - In this scenario the opinion of 4 friends who have experience are weighted higher as compared to the rest of the 3.\n",
    "        \n",
    "            F1 - IPhone\n",
    "            F2 - Redmi\n",
    "            F3 - Oppo\n",
    "            F4 - Vivo\n",
    "            F5 - Samsung\n",
    "            F6 - One Plus\n",
    "            F7 - One Plus\n",
    "            \n",
    "                  F1   F2   F3   F4   F5   F6   F7\n",
    "        Weight:  0.23 0.23 0.23 0.23 0.21 0.10 0.10\n",
    "        Rating:   1    2    2    3    4    5    5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0602308",
   "metadata": {},
   "source": [
    "### Hard Voting Vs Soft Voting\n",
    "\n",
    "**Hard Voting:-**\n",
    "\n",
    "    - In classification a voting ensemble involves making a prediction.\n",
    "    - A Hard voting ensemble involves summing the votes for crisp and labelled data.\n",
    "    - It also involves summing up all the votes and predicting with the most vote.\n",
    "    - Typically used in numerical & categorical data.\n",
    "\n",
    "**Soft Voting:-**\n",
    "\n",
    "    - In soft voting ensemble involves summing up the predicted probablities.\n",
    "    - Typically used in classed labeled data.\n",
    "    - It predicts class with the largest summed of probablity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3dc05",
   "metadata": {},
   "source": [
    "## Advanced Technique In Ensemble Learning Algorithm\n",
    "\n",
    "### 1) Stacking:-\n",
    "\n",
    "    a). Stacking also known as \"Stacked Generalization\" is an ensemble technique that combines multiple classification or\n",
    "        regression model via meta-classifier or meta-regressor.\n",
    "    b). The base level model are trained on a complete training set then the meta model id trained on the features that are\n",
    "        output of the base level model.\n",
    "    c). The base level often consists of different learning algorithm and therefore stacking algorithm is often\n",
    "        heterogenous(different).\n",
    "\n",
    "**Meta Classifier:-**\n",
    "        \n",
    "    - It is a classifier that makes a final prediciton among all the predictions by using those predictions as a feature.\n",
    "    - It takes classes by various classifiers and pick the final one as the result.\n",
    "        \n",
    "**Meta Regressor:-**\n",
    "        \n",
    "    - Meta regressor is defined to be a meta analysis to combine, compare and synthesize research findings.\n",
    "    - A meta regressor analysis aim to reconcile conflicting studies or colaborate consisting once.\n",
    "    - It combines the data of multiple studies to identify overall trend of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08eab2",
   "metadata": {},
   "source": [
    "## Example : Max Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6707d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "998ed111",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:,1:3]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a28c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = KNeighborsClassifier()\n",
    "clf4 = GaussianNB()\n",
    "clf5 = DecisionTreeClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "527f68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.95 (+/- 0.04)[Logistic]\n",
      "Accuracy Score: 0.94 (+/- 0.04)[Random Forest]\n",
      "Accuracy Score: 0.95 (+/- 0.04)[KNN]\n",
      "Accuracy Score: 0.91 (+/- 0.04)[Gaussian]\n",
      "Accuracy Score: 0.91 (+/- 0.03)[Decision Tree]\n"
     ]
    }
   ],
   "source": [
    "labels = ['Logistic', 'Random Forest', 'KNN', 'Gaussian' ,'Decision Tree']\n",
    "for clf,label in zip([clf1, clf2, clf3, clf4, clf5], labels):\n",
    "    score = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(\"Accuracy Score: %0.2f (+/- %0.2f)[%s]\"%(score.mean(), score.std(),label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37dae716",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(estimators=[\n",
    "    (labels[0],clf1),\n",
    "    (labels[1],clf2),\n",
    "    (labels[2],clf3),\n",
    "    (labels[3],clf4),\n",
    "    (labels[4],clf5)],voting='hard')\n",
    "\n",
    "voting_clf_soft = VotingClassifier(estimators=[\n",
    "    (labels[0],clf1),\n",
    "    (labels[1],clf2),\n",
    "    (labels[2],clf3),\n",
    "    (labels[3],clf4),\n",
    "    (labels[4],clf5)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d716b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.95 (+/- 0.04)[Logistics]\n",
      "Accuracy Score: 0.94 (+/- 0.04)[Random Forest]\n",
      "Accuracy Score: 0.95 (+/- 0.04)[KNN]\n",
      "Accuracy Score: 0.91 (+/- 0.04)[Gaussian]\n",
      "Accuracy Score: 0.91 (+/- 0.03)[Decision Tree]\n",
      "Accuracy Score: 0.95 (+/- 0.04)[Hard Voting]\n",
      "Accuracy Score: 0.95 (+/- 0.04)[Soft Voting]\n"
     ]
    }
   ],
   "source": [
    "labels_new = ['Logistics','Random Forest', 'KNN', 'Gaussian', 'Decision Tree', 'Hard Voting','Soft Voting']\n",
    "for (clf,label) in zip([clf1, clf2, clf3, clf4, clf5, voting_clf_hard, voting_clf_soft],labels_new):\n",
    "    scores = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(\"Accuracy Score: %0.2f (+/- %0.2f)[%s]\"%(scores.mean(),scores.std(),label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
